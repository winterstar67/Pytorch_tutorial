{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b162384-05e6-4adf-920e-2bba655d1ab6",
   "metadata": {},
   "source": [
    "# Pytorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ce145-1690-42c2-aa25-92f22732b5e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. OPTIMIZING MODEL PARAMETERS\n",
    "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a3987-0202-4b5c-bccc-d4ccdd5cc9d7",
   "metadata": {},
   "source": [
    "모델과 데이터가 있다 할때, parameters를 사용자가 가진 data에 맞게 optimize를 하여 모델을 train, validate, test하고자 한다.  \n",
    "model training은 반복적인 process이다. 각 iteration에서 모델은 output을 계산하고, output으로부터 loss(error)를 측정하여, parameters에 대한 loss의 미분을 collect한다. 그 후, 이러한 parameters를 gradient descent를 통해 optimize한다.  \n",
    "이 process의 더 구체적인 내용으로는  [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8) 를 참고하라고 나와있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85dffeb-0152-414b-baa0-fa2b707252bd",
   "metadata": {},
   "source": [
    "`-` 시작전 준비 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ef137f-a32c-42a7-a967-d2794dd946ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # Model build에 필요한 building block을 제공해주는 라이브러리\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # root directory\n",
    "    train = True, # \n",
    "    download = True, # 인터넷에서 다운로드 받아 root directory에 저장\n",
    "    transform=ToTensor() # 데아터를 tensor로 변환\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e6fd8-50e7-4b52-a3a5-561a14a4b3e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc25cbe-c1c8-4ad8-89e5-ff90161cae9c",
   "metadata": {},
   "source": [
    "Hyperparameters는 사용자가 모델 optimization 과정을 조절하도록 해주는 parameters이다. hyperparameters가 달라짐에 따라 model training과 convergence rates에 영향을 줄 수 있다.\n",
    "- Hyperparameter tuning 관련 글: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "\n",
    "여기서, 다음의 hyperparameters를 training을 위해 설정한다:\n",
    "- Epochs의 수: Datset에 대해 iterate를 할 횟수\n",
    "- Batch size: Parameters 업데이트 전에 network를 propagate하게 될 데이터 samples의 개수\n",
    "- Learning rate(LR): 힌번의 epoch(혹은 batch)마다 모델 parameters를 얼마나 업데이트 할지, 작은 LR값은 learning speed가 느리다. 반면에 큰 값은 training동안 예상치못한 결과를 낼수도 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55aababf-fb8d-4e50-bafe-8e1e361d4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258dc75c-8ef2-4f51-8d68-0cf255565136",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507d23d-5f7b-4d51-a8cb-8689f0e92d61",
   "metadata": {},
   "source": [
    "사용자가 hyperparameters를 설정하면, 다음으로 optimization loop를 통해 model을 training하고 optimization 할 수 있다.  \n",
    "optimization loop의 각각 iteration은 epoch 한번이라 불린다.  \n",
    "각 Epoch은 2개의 부분으로 이루어져있다.\n",
    "- The Train Loop - training dataset에 대해서 iterate를 하고 training data에 대한 최적의 parameters로 수렴하려 한다.\n",
    "- The Validation/Test Loop: valid/test dataset에 대해서 iterate를 하며, 모델 성능이 향상되고 있는지 확인하기 위해 사용된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862e68b-7b1a-40a5-ab7c-736a7bc18c6c",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eeff69-1eba-4aaf-a59b-ed94c7473c08",
   "metadata": {},
   "source": [
    "training data가 제시되었을 때, untrained된 network는 정확한 답을 주지 않을 것이다.  \n",
    "이 때 Loss function이 실제 답과 model이 예측한 값 사이의 틀린 정도를 측정해준다. 그리고 model training에서 이 loss function 값을 minimize 하는 것이 목표이다.  \n",
    "loss를 계산하기 위해, 사용자는 input을 모델에 입력해서 prediction을 만들고 이를 실제 data label과 비교함으로써 얻을 수 있다.\n",
    "\n",
    "흔히 사용되는 loss function의 예시는 다음과 같다. \n",
    "- regression task에 쓰이는 `nn.MSELoss`(Mean Squre Error) \n",
    "- classification에 쓰이는 `nn.NLLLoss` (Negative Log Likelihood)\n",
    "- `nn.Softmax`, `nn.NLLLoss`를 합친 `nn.CrosEntropyLoss`\n",
    "\n",
    "여기서는 logis를 normalize하고 prediction error를 측정하는 `nn.CrossEntropyLoss`에 model output을 입력한다.\n",
    "- logits는 0과 1사이의 값으로 변환된 값이라고 보면 될 것 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525cdab0-762b-4499-8914-84a5b5676f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f204c1e-7e54-4ff4-a51b-b3e30248e6ac",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556071dc-7351-4a69-b597-896ab27d3f9c",
   "metadata": {},
   "source": [
    "Optimization은 model parameters를 조정하여 training step에서 model error를 줄이기위한 과정이다. `Optimization algorithm`은 어떻게 이러한 과정이 수행되는지 정의한다(이번 예시에서는 Stochastic Gradient Descent를 사용한다). 모든 optimization logic은 `otimizer` object에 담겨있다. optimizer는 SGD optimizer말고도 ADAM, RMSProp와 같은 다양한 optimizer가 pytorch에 있다.  \n",
    "사용자는 먼저 training 되어야할 model의 parameters를 optimizer에 registering함으로써 optimizer를 initialize한다. 그리고 learning rate역시 optimizer에 건내준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b18fea-7527-4d80-aac1-1ecd8529f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200989b-225f-4a53-91e6-f9115c5dfbf3",
   "metadata": {},
   "source": [
    "training loop 내부에서 optimization은 다음 3가지 steps으로 이루어진다:\n",
    "- `optimizer.zero_grad()`를 호출해서 model parameters의 gradient를 reset한다. Gradients는 default로 더해지면서 쌓이게 되는데, 이걸 방지하기 위해 사용자는 각 iteration마다 zero로 만드는 것을 명시한다.\n",
    "- `loss.backward()`를 호출하여 prediction loss를 Backpropagate한다. Pytorch는 각 parameters의 loss에 대한 gradient를 축적한다.\n",
    "- Gradients를 계산한 다음, `optimizer.step()`를 호출하여 backward에서 계산된 gradients를 기반으로 parameters를 수정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5407a44-4239-4c94-9d38-5082ca4384dd",
   "metadata": {},
   "source": [
    "## 전체 구현 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96ec08-c22e-41ae-a202-e16c1012386f",
   "metadata": {},
   "source": [
    "train_loop를 정의하여 optimization loop를 구현하고, test_loop로 test data에 대한 model performance를 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce46895d-db60-4b52-8d77-2216482572c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        \n",
    "        optimizer.zero_grad() # backward에 문제가 없도록 먼저 zero_grad를 실행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b904d4-3351-4582-85d3-0b41121da1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303218  [   64/60000]\n",
      "loss: 2.291858  [ 6464/60000]\n",
      "loss: 2.273129  [12864/60000]\n",
      "loss: 2.263830  [19264/60000]\n",
      "loss: 2.244684  [25664/60000]\n",
      "loss: 2.212411  [32064/60000]\n",
      "loss: 2.225910  [38464/60000]\n",
      "loss: 2.188714  [44864/60000]\n",
      "loss: 2.175487  [51264/60000]\n",
      "loss: 2.147411  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.149823 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.160450  [   64/60000]\n",
      "loss: 2.155108  [ 6464/60000]\n",
      "loss: 2.097090  [12864/60000]\n",
      "loss: 2.106639  [19264/60000]\n",
      "loss: 2.060651  [25664/60000]\n",
      "loss: 1.994428  [32064/60000]\n",
      "loss: 2.023624  [38464/60000]\n",
      "loss: 1.942107  [44864/60000]\n",
      "loss: 1.941576  [51264/60000]\n",
      "loss: 1.873218  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.878297 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.911321  [   64/60000]\n",
      "loss: 1.887351  [ 6464/60000]\n",
      "loss: 1.769241  [12864/60000]\n",
      "loss: 1.803301  [19264/60000]\n",
      "loss: 1.698864  [25664/60000]\n",
      "loss: 1.645900  [32064/60000]\n",
      "loss: 1.667354  [38464/60000]\n",
      "loss: 1.568418  [44864/60000]\n",
      "loss: 1.591409  [51264/60000]\n",
      "loss: 1.489412  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.510613 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.576304  [   64/60000]\n",
      "loss: 1.545629  [ 6464/60000]\n",
      "loss: 1.395561  [12864/60000]\n",
      "loss: 1.465202  [19264/60000]\n",
      "loss: 1.347158  [25664/60000]\n",
      "loss: 1.338648  [32064/60000]\n",
      "loss: 1.355027  [38464/60000]\n",
      "loss: 1.279993  [44864/60000]\n",
      "loss: 1.314030  [51264/60000]\n",
      "loss: 1.219460  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.245544 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.318799  [   64/60000]\n",
      "loss: 1.304709  [ 6464/60000]\n",
      "loss: 1.139781  [12864/60000]\n",
      "loss: 1.246775  [19264/60000]\n",
      "loss: 1.119074  [25664/60000]\n",
      "loss: 1.138494  [32064/60000]\n",
      "loss: 1.164043  [38464/60000]\n",
      "loss: 1.101185  [44864/60000]\n",
      "loss: 1.140625  [51264/60000]\n",
      "loss: 1.062317  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.081414 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.145752  [   64/60000]\n",
      "loss: 1.154121  [ 6464/60000]\n",
      "loss: 0.972065  [12864/60000]\n",
      "loss: 1.110845  [19264/60000]\n",
      "loss: 0.979169  [25664/60000]\n",
      "loss: 1.003785  [32064/60000]\n",
      "loss: 1.047273  [38464/60000]\n",
      "loss: 0.987099  [44864/60000]\n",
      "loss: 1.027955  [51264/60000]\n",
      "loss: 0.964689  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.976151 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.026487  [   64/60000]\n",
      "loss: 1.057805  [ 6464/60000]\n",
      "loss: 0.857745  [12864/60000]\n",
      "loss: 1.020870  [19264/60000]\n",
      "loss: 0.892236  [25664/60000]\n",
      "loss: 0.909409  [32064/60000]\n",
      "loss: 0.971846  [38464/60000]\n",
      "loss: 0.912917  [44864/60000]\n",
      "loss: 0.949967  [51264/60000]\n",
      "loss: 0.900101  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.904804 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.939691  [   64/60000]\n",
      "loss: 0.991242  [ 6464/60000]\n",
      "loss: 0.775922  [12864/60000]\n",
      "loss: 0.957739  [19264/60000]\n",
      "loss: 0.834524  [25664/60000]\n",
      "loss: 0.841304  [32064/60000]\n",
      "loss: 0.919127  [38464/60000]\n",
      "loss: 0.862968  [44864/60000]\n",
      "loss: 0.893346  [51264/60000]\n",
      "loss: 0.853826  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.853692 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.873539  [   64/60000]\n",
      "loss: 0.941764  [ 6464/60000]\n",
      "loss: 0.714980  [12864/60000]\n",
      "loss: 0.911144  [19264/60000]\n",
      "loss: 0.793387  [25664/60000]\n",
      "loss: 0.790999  [32064/60000]\n",
      "loss: 0.879291  [38464/60000]\n",
      "loss: 0.827773  [44864/60000]\n",
      "loss: 0.850666  [51264/60000]\n",
      "loss: 0.818178  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.815107 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.820892  [   64/60000]\n",
      "loss: 0.902402  [ 6464/60000]\n",
      "loss: 0.667441  [12864/60000]\n",
      "loss: 0.875189  [19264/60000]\n",
      "loss: 0.762070  [25664/60000]\n",
      "loss: 0.753053  [32064/60000]\n",
      "loss: 0.847111  [38464/60000]\n",
      "loss: 0.801741  [44864/60000]\n",
      "loss: 0.817223  [51264/60000]\n",
      "loss: 0.789394  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.784374 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
