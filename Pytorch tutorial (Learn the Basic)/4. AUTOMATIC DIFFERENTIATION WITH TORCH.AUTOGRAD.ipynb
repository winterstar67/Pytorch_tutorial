{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849ce145-1690-42c2-aa25-92f22732b5e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c2779-8dd5-4f0c-8286-018df7b4e68e",
   "metadata": {},
   "source": [
    "NN(Neural Network)을 training 할 때, 가장 많이 쓰이는 알고리즘이 `back propagation`이다.\n",
    "Pytorch는 `torch.autograd`라고 하는 내장된 미분 기능을 제공한다. 이는 어떤 computational graph에 대해서도 자동으로 미분을 계산해준다.\n",
    "- `back propagation` 알고리즘은 loss function의 gradient에 따라 모델의 파라메터를 조정하는 일을 한다.\n",
    "- `computational graph`는 계산 과정을 그래프로 나타낸 것이다. (https://compmath.korea.ac.kr/appmath2021/BackPropagation.html)\n",
    "\n",
    "가장 간단한 1층 레이어 NN를 보자. 여기서 input은 x, parameters는 w와 b로 나타난다. 이는 Pytorch에서 아래와 같이 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0731f4d3-7f58-4840-98c9-a1443968ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch library 불러오기\n",
    "\n",
    "x = torch.ones(5) # input에 해당하는 tensor\n",
    "y = torch.zeros(3) # 실제 label 역할을 하는 tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights에 해당하는 parameter\n",
    "b = torch.randn(3, requires_grad=True) # bias에 해당하는 parameter\n",
    "z = torch.matmul(x, w) + b # x와 w를 matrix multiplication을 한 후, bias를 더한다.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45915b6c-6662-416d-9a81-cbde170e47ad",
   "metadata": {},
   "source": [
    "- requires_grad = True로 되어있는 tensor는, 이 텐서에서 이루어진 모든 연산을 기록하게 된다고 한다. (https://deepinsight.tistory.com/84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263ac61e-14a7-49c1-b1b8-84749866bbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x와 w의 shape에 관해서, torch.matmul(x, w)가 계산이 되었음.\n",
    "x.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41140a06-9c65-49b6-a2c9-269ef13f037a",
   "metadata": {},
   "source": [
    "- ones(n)은 길이가 n인 row로 여겨지는 듯 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686ddc9-4ff7-4b24-a8af-be701eb75a61",
   "metadata": {},
   "source": [
    "## Tensors, Functions and Computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41356d26-4dd1-4baa-81ba-9f0d44514932",
   "metadata": {},
   "source": [
    "위에서 작성한 코드는 아래의 computational graph를 정의한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12ca06-7829-4270-b74a-c80ec26ca28d",
   "metadata": {},
   "source": [
    "![Computational graph](https://pytorch.org/tutorials/_images/comp-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fb03f-0d1b-4c30-bbcc-5cb150779e9c",
   "metadata": {},
   "source": [
    "이 network에서 w와 b는 최적화 과정을 통해 값이 변화되어야 하는 파라메터이다. 이를 위해, w와 b에 대한 loss function의 gradient를 계산할 수 있어야한다.  \n",
    "이렇게 loss function을 w와 b 각각에 대해서 미분하고 gradient를 구하기 위해, `requires_grad`라는 tensor의 property를 설정할 수 있다.\n",
    "- `requires_grad`라는 인자는 tensor를 생성할 때 정의하거나, `tensor.requires_grad_(True)` method를 사용해서 설정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094b4ba-af2e-4862-b10e-0cfbb24554c2",
   "metadata": {},
   "source": [
    "위에서 적욘된 `binary_cross_entropy_with_logits`는 Function 클래스의 함수이다. 이 함수는 forward computation을 진행하고 backward propagation 동안 미분을 계산한다. 여기서 backward propagation 함수에 대한 reference는 텐서의 grad_fn에 저장되어있다.  \n",
    "아래 코드를 통해서 grad_fn을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc7b838d-cb4e-4dce-802a-b342aabd682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000027802F676A0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000027809F4FEB0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5880476-5450-4ab2-8106-d2faadbe668c",
   "metadata": {},
   "source": [
    "## Gradients 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d67bb-45eb-4139-aebb-9046d918f258",
   "metadata": {},
   "source": [
    "우리의 목적은 NN의 parameters를 최적화하는 것이다. 따라서, parameters에 대한 loss function의 미분, 즉 weights에 대한 $\\frac{\\partial Loss}{\\partial w}$ 와 bias에 대한 $\\frac{\\partial Loss}{\\partial b}$를 계산할 필요가 있다.  \n",
    "이러한 미분 계산을 위해, `loss.backward()`라는 함수를 사용하고, 이로부터 `w.grad`와 `b.grad`를 통해서 미분 값을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a817294a-10b9-44db-9e5c-082482c6d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of w:  tensor([[0.2100, 0.2174, 0.1833],\n",
      "        [0.2100, 0.2174, 0.1833],\n",
      "        [0.2100, 0.2174, 0.1833],\n",
      "        [0.2100, 0.2174, 0.1833],\n",
      "        [0.2100, 0.2174, 0.1833]])\n",
      "Derivative of b:  tensor([0.2100, 0.2174, 0.1833])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"Derivative of w: \",w.grad)\n",
    "print(\"Derivative of b: \",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e66b6e-df22-41dd-bd36-52210a15a07e",
   "metadata": {},
   "source": [
    "- Computational graph 상에서, requires_grad = True로 설정된 leaf node만이 `grad` property 만을 얻을 수 있다. graph의 다른 노드들에 대해서는 gradident가 불가능하다.\n",
    "- `backward`를 통한 Gradient 계산은 성능 문제로 한번씩만 가능하다. 만약 같은 graph에 대해서 여러번 `backward`를 해야한다면, `retain_graph=True`로 설정한 다음 `backward`를 호출한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25515af4-54fb-4210-b1c6-fe267e41c3a1",
   "metadata": {},
   "source": [
    "## Gradient tracking 멈추기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9ce5f-70a7-4b51-a185-02e65f1910c0",
   "metadata": {},
   "source": [
    "Default로 `requires_grad = True`로 텐서는 이들의 계산한 기록들을 tracking하고 gradient 계산을 할 수 있게 도와준다. 하지만, 이것이 모든 경우에 필요한 것은 아니다. 예를들어, 모델이 학습되고 이를 어떤 input data에 적용하고자 할 때, 즉 `forward` 계산만 하고싶은 상황이라 하자. 이 때, `torch.no_grad()`를 통해서 computation tracking(기록)을 멈출 수 있다.\n",
    "\n",
    "아래 코드는 requires_grad를 False로 바꾸고, 이것이 잘 작동하는지 확인하는 코드이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a62d8e-d973-432b-975f-154b6aea3f4c",
   "metadata": {},
   "source": [
    "- 그런데 왜 tracking을 멈춰야 하는 것일까? forward 때도 굳이 일일이 `torch.no_grad()`를 선언하지 않고, Gradient tracking을 하게 놔둬도 괜찮지 않을까?\n",
    "    - gradient tracking에는 메모리 소모가 심한듯 하다(출처: 딥러닝 공부방)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0be8615-f55c-4abe-9411-25622f967344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ec3bc-e152-4764-9fc4-d3b055794f70",
   "metadata": {},
   "source": [
    "- 저 `with` 내부에 있는 코드는 w와 b라는 require_grad=True가 있어도 그 연산 결과를 저장하는 것은 requires_grad = False로 해주는 코드인듯 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a14fef-096f-4535-8593-394ab261eb78",
   "metadata": {},
   "source": [
    "위와 같은 결과는 tensor에 `detach()` 함수를 사용해서도 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0272110f-02bd-4d3b-a465-fc84419d96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before detach():  True\n",
      "After detach():  False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(\"Before detach(): \",z.requires_grad)\n",
    "z_det = z.detach()\n",
    "print(\"After detach(): \",z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c54e25-2e82-4a7a-95c3-fd6441643e0c",
   "metadata": {},
   "source": [
    "Gradient tracking을 멈추는 것을 고려해볼만한 경우:\n",
    "1. 모델 내의 몇몇 parameters를 frozen하고싶은 경우\n",
    "2. Forward 연산만을 수행할 때 계산 속도를 올리고 싶은경우이다. 속도가 올라가는 이유는 gradient를 tracking하지 않은 tensor에 대한 계산이 더 효율적이기 때문이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ec498-b45a-4ecb-b28e-0e65f3fcea89",
   "metadata": {},
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcc38d-79ee-44ea-b693-c077f43be0da",
   "metadata": {},
   "source": [
    "개념적으로, autograd가 데이터(tensor)와 모든 수행되는 연산(새로운 텐서 결과에 따라)의 기록을 Directed acyclic graph(DAG) 계속 유지한다. DAG에서는 leaves가 input tensor이고 roots는 output tensors이다. 이 graph를 roots부터 leaves까지 tracking함으로써, 자동으로 `chain rule`을 적용하여 graidents를 계산할 수 있게된다.  \n",
    "\n",
    "Forward 계산을 할 때, autograd는 다음 두 가지를 동시에 한다:\n",
    "- 사용자가 요청한 operation을 실행하여 결과 tensor를 계산한다.\n",
    "- 실행된 operation의 gradient 함수를 DAG에 유지한다.\n",
    "\n",
    "Backward 계산은 `.backward()`가 DAG의 root에서 호출됬을 때 시작된다. autograd 에서는:\n",
    "- gradients를 각각의 `.grad_fn`에서부터 계산한다.\n",
    "- 각각 텐서의 `.grad` 속성에 계산된 gradients를 accumulates한다.\n",
    "- chain rule을 사용하여, 모든 leaf tensors까지 propagate(전달)한다.\n",
    "\n",
    "\n",
    "`*` DAG는 튜토리얼 처음에 나온 그래프와 같은 경우로 보면 될듯하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27594c2-bbd1-4231-aceb-584aecdd1199",
   "metadata": {},
   "source": [
    "`Note`  \n",
    "Pytorch에서 DAG는 dynamic하다. 여기서 알아둘만한 것은 Graph는 scratch부터 다시 생성된다는 것이다.  \n",
    "각각의 `.backward()` 호출을 한 후에, autograd는 새로운 graph를 채우기 시작한다. 이는 사용자가 모델에 control flow statements를 사용할 수 있도록 해준다. 만약 필요하다면 사용자는 shape, size와 연산을 매 iteration마다 바꿀 수 있다 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca8227-efce-4c3c-8009-a6d600ad748b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0cbcb6-1580-45c3-b6cd-57722a6fa896",
   "metadata": {},
   "source": [
    "## 튜토리얼 외의 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fea8c-96d7-447a-8ecd-3aacafbbaa05",
   "metadata": {},
   "source": [
    "`-` retain_graph 란?\n",
    "\n",
    "해당 내용을 조사해 보았다.  \n",
    "먼저, **backward는 한번 씩만 가능하다.** 의 의미는 위의 loss.backward()를 연속으로 호출할 수 없다는 의미이다.  \n",
    "아래 코드를 보자\n",
    "\n",
    "- 참고한 블로그:\n",
    "    - https://hongl.tistory.com/158\n",
    "    - https://m.blog.naver.com/egg5562/221285463744\n",
    "    - https://deep-learning-study.tistory.com/305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb24abf2-bc18-4bea-8c18-35397237695a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10332\\656307769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neogreen\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neogreen\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f413c5-d1b5-49b5-9171-7663cd90f8eb",
   "metadata": {},
   "source": [
    "- graph에 대한 두 번째 backward를 실행하면 중간의 저장된 값들이 free가 된다.\n",
    "- 만약 backward를 두번 해야한다면, retain_graph=True로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116e685-1a0b-42ae-9ff2-ec649b8ab02f",
   "metadata": {},
   "source": [
    "위에서 x, y, w, b, z, loss를 정의하였고, 이들은 computational graph상에서 모두 연결되어 있는 값들이다. 계산의 진행 방향은 x,w -> b -> z -> y -> loss를 방향으로 연결되어있다.\n",
    "\n",
    "여기서 loss.backward()를 하여 loss에 대한 미분을 계산하면, loss까지 연결된 x,w,b,z,y,loss 값들을 모두 지워진다는 의미가 된다.  \n",
    "\n",
    "그런데, 위에서 나온 `retain_graph=True`를 설정하면 graph의 중간 노드값들이 지워지지 않고 그래프가 유지된다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd35474-18c5-4d06-9643-1db60902afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5) # input에 해당하는 tensor\n",
    "y = torch.zeros(3) # 실제 label 역할을 하는 tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights에 해당하는 parameter\n",
    "b = torch.randn(3, requires_grad=True) # bias에 해당하는 parameter\n",
    "z = torch.matmul(x, w) + b # x와 w를 matrix multiplication을 한 후, bias를 더한다.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac79e81-d423-46ac-b17f-a6f317a9f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aaf08-989f-49dd-b3ba-46ea0ef5a9ee",
   "metadata": {},
   "source": [
    "- 이렇게 하면 에러가 일어나지 않게된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b31fb-0440-4a77-a8e3-390773c5e8f0",
   "metadata": {},
   "source": [
    "`-` 필자는 retain_graph를 접해본 적이 없어서 언제 사용할까 궁금했는데, 참고 블로그에서 해당 경우를 작성해주셨다.  \n",
    "해당 내용을 정리해보면,\n",
    "\n",
    "각 층에 대한 여러 개의 목적함수가 존재하는 multi task learning에 자주 사용된다. 예를들어, loss1, loss2가 있을 때, 각 loss에 대한 기울기를  \n",
    "\n",
    "loss1.backward(retain_graph=True)\n",
    "loss2.backward()\n",
    "\n",
    "로 계산할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dc6d9-17af-49aa-a731-5b4f8a1cce9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0b205-75aa-4cc1-89c4-a3300684c129",
   "metadata": {},
   "source": [
    "`-` Gradient 값이 축적되는지 확인하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c3c01-0506-49b3-be85-0a003d3c10cc",
   "metadata": {},
   "source": [
    "`-` backward()를 한번만 한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2826f0ba-da87-42ab-bdf9-737c369b0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of w:  tensor([[0.1083, 0.0369, 0.0726],\n",
      "        [0.1083, 0.0369, 0.0726],\n",
      "        [0.1083, 0.0369, 0.0726],\n",
      "        [0.1083, 0.0369, 0.0726],\n",
      "        [0.1083, 0.0369, 0.0726]])\n",
      "Derivative of b:  tensor([0.1083, 0.0369, 0.0726])\n"
     ]
    }
   ],
   "source": [
    "import torch # pytorch library 불러오기\n",
    "\n",
    "torch.manual_seed(1) # 항상 같은 코드에 대해서는 같은 결과가 나오도록 고정\n",
    "\n",
    "x = torch.ones(5) # input에 해당하는 tensor\n",
    "y = torch.zeros(3) # 실제 label 역할을 하는 tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights에 해당하는 parameter\n",
    "b = torch.randn(3, requires_grad=True) # bias에 해당하는 parameter\n",
    "z = torch.matmul(x, w) + b # x와 w를 matrix multiplication을 한 후, bias를 더한다.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Derivative of w: \",w.grad)\n",
    "print(\"Derivative of b: \",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91949a0e-c4ae-4b19-8925-d09d2f23362d",
   "metadata": {},
   "source": [
    "`-`  backward()를 두번한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ad70bfc-22c8-4ff2-9f7a-64406ecdb587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of w:  tensor([[0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452]])\n",
      "Derivative of b:  tensor([0.2166, 0.0739, 0.1452])\n"
     ]
    }
   ],
   "source": [
    "import torch # pytorch library 불러오기\n",
    "\n",
    "torch.manual_seed(1) # 항상 같은 코드에 대해서는 같은 결과가 나오도록 고정\n",
    "\n",
    "x = torch.ones(5) # input에 해당하는 tensor\n",
    "y = torch.zeros(3) # 실제 label 역할을 하는 tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights에 해당하는 parameter\n",
    "b = torch.randn(3, requires_grad=True) # bias에 해당하는 parameter\n",
    "z = torch.matmul(x, w) + b # x와 w를 matrix multiplication을 한 후, bias를 더한다.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Derivative of w: \",w.grad)\n",
    "print(\"Derivative of b: \",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09beba5-a71b-42a6-b1cc-4ec09912770f",
   "metadata": {},
   "source": [
    "`-` 만약 축적이 된다면, 이는 `loss.backward()`를 한 결과에 *2를 한 값과 동일할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce82491b-f477-42ed-9311-2ea9085422ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of w:  tensor([[0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452],\n",
      "        [0.2166, 0.0739, 0.1452]])\n",
      "Derivative of b:  tensor([0.2166, 0.0739, 0.1452])\n"
     ]
    }
   ],
   "source": [
    "import torch # pytorch library 불러오기\n",
    "\n",
    "torch.manual_seed(1) # 항상 같은 코드에 대해서는 같은 결과가 나오도록 고정\n",
    "\n",
    "x = torch.ones(5) # input에 해당하는 tensor\n",
    "y = torch.zeros(3) # 실제 label 역할을 하는 tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights에 해당하는 parameter\n",
    "b = torch.randn(3, requires_grad=True) # bias에 해당하는 parameter\n",
    "z = torch.matmul(x, w) + b # x와 w를 matrix multiplication을 한 후, bias를 더한다.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Derivative of w: \",w.grad*2)\n",
    "print(\"Derivative of b: \",b.grad*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7ad41-a65d-469f-a41f-c963abc7f9f6",
   "metadata": {},
   "source": [
    "- 기존 loss.backward()에 *2를 한 것과 retain_graph=True로 backward를 두번한 결과는 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc708f-6d79-461b-b6b6-80f7d6431ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbb38b8-4175-4268-80ea-9ee945fb9033",
   "metadata": {},
   "source": [
    "`-` 이번에는 Tutorial 5의 optimizer에서 zero_grad를 했을 때, 축적되는 것이 없어지는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ae905a63-9cea-441d-a63f-76eb30c72d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # Model build에 필요한 building block을 제공해주는 라이브러리\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # root directory\n",
    "    train = True, # \n",
    "    download = True, # 인터넷에서 다운로드 받아 root directory에 저장\n",
    "    transform=ToTensor() # 데아터를 tensor로 변환\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e05a3-faee-4d2b-9da0-7e009386a13d",
   "metadata": {},
   "source": [
    "`-` optimizer.zero_grad() 를 한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "96bbbdeb-c5e8-44c4-8e24-687729e2c7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "1번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "2번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "3번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "4번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "5번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "6번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "7번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "8번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "9번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Compute prediction and loss\n",
    "    for i in range(10):\n",
    "        X, y = training_data[1012]\n",
    "        y = torch.tensor(y).reshape([1])\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(f\"{i}번째 loss.backward()\",list(model.parameters())[3].grad[0:6])\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "train_loop(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6c579-7813-4cf4-888b-2f23c8e9a71e",
   "metadata": {},
   "source": [
    "`-` optimizer.zero_grad() 를 하지 않은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed6047-98ca-4c06-9ed8-5797bd21b7e2",
   "metadata": {},
   "source": [
    "retrain_graph=True를 하지 않아도 되는 이유는 loop마다 X,y를 다시 정의해서 computational graph가 새로 생성된 형태여서 그런듯.  \n",
    "하지만, 모델의 파라메터 자체는 사라지고 생기는 것 없이 계속 유지되므로, w나 b 혹은 model parameters에 저장되어있는 gradient 값은 계속해서 축적이 되는 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7a2b57d2-6eca-42b2-8e9b-d02492050b55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "1번째 loss.backward() tensor([0.0000, 0.0000, 0.0045, 0.0345, 0.0000, 0.0894])\n",
      "2번째 loss.backward() tensor([0.0000, 0.0000, 0.0068, 0.0518, 0.0000, 0.1341])\n",
      "3번째 loss.backward() tensor([0.0000, 0.0000, 0.0090, 0.0690, 0.0000, 0.1788])\n",
      "4번째 loss.backward() tensor([0.0000, 0.0000, 0.0112, 0.0862, 0.0000, 0.2235])\n",
      "5번째 loss.backward() tensor([0.0000, 0.0000, 0.0135, 0.1035, 0.0000, 0.2681])\n",
      "6번째 loss.backward() tensor([0.0000, 0.0000, 0.0157, 0.1207, 0.0000, 0.3127])\n",
      "7번째 loss.backward() tensor([0.0000, 0.0000, 0.0179, 0.1379, 0.0000, 0.3572])\n",
      "8번째 loss.backward() tensor([0.0000, 0.0000, 0.0202, 0.1552, 0.0000, 0.4017])\n",
      "9번째 loss.backward() tensor([0.0000, 0.0000, 0.0224, 0.1724, 0.0000, 0.4461])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Compute prediction and loss\n",
    "    for i in range(10):\n",
    "        X, y = training_data[1012]\n",
    "        y = torch.tensor(y).reshape([1])\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        print(f\"{i}번째 loss.backward()\",list(model.parameters())[3].grad[0:6])\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "train_loop(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91f578-04fd-48b6-8821-0dda658c5550",
   "metadata": {},
   "source": [
    "`-` optimizer.step() 를 하지 않은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f029f93a-8daf-450f-8be1-f10f5d152527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "1번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "2번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "3번째 loss.backward() tensor([0.0000, 0.0000, 0.0023, 0.0173, 0.0000, 0.0447])\n",
      "4번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "5번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "6번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "7번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "8번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n",
      "9번째 loss.backward() tensor([0.0000, 0.0000, 0.0022, 0.0172, 0.0000, 0.0447])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Compute prediction and loss\n",
    "    for i in range(10):\n",
    "        X, y = training_data[1012]\n",
    "        y = torch.tensor(y).reshape([1])\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(f\"{i}번째 loss.backward()\",list(model.parameters())[3].grad[0:6])\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "train_loop(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede698c-54d8-457b-a535-f24af558606b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "773eb4d4-86a2-4d27-a6f9-26b9e907c035",
   "metadata": {},
   "source": [
    "결과를 정리해보면, retain_graph=True는 gradient 계산 후에 사라진 computational graph를 복구시키는 것이지만, 저장되있는 gradient에는 영향을 주지 않는다.  \n",
    "반면에 zero_grad()는 저장되어있는 gradient를 초기화시키는 역할을 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
